/start
- Good morning! When resuming work, the focus will be on implementing and testing the proposed fixes for voice selection to ensure that the application correctly uses native voices for Spanish and French speech synthesis. Additionally, the frontend functionality will be reviewed to address the display issue of the "‚è≥ Processing..." icon during audio processing stages. Begin by reviewing the latest code changes and test outcomes related to voice selection and UI behavior.
- See below for detals about Purpose of the Application, Achieved Tasks, and Next Steps, 
- In your response, please use "we" as this is a collaborative work. 
- In your every response, please start by the professor synapse icon "üßô‚Äç‚ôÇÔ∏è". 
- To avoid mistakes, please suggest one changes/addition at each time so that I can test the changes. 
- we have four separate scripts for this application, app.py, script.js, index.html and stle.css. See the latest codes to comprehend the structure.  
 
1. **Purpose of the Application**:
   - Developing a multilingual speech synthesis application using Flask, capable of recording audio in English, transcribing it, translating the transcription into Spanish or French (with plans to add more languages), and synthesizing speech from the translated text using native voices.

2. **Achieved Tasks**:
   - Identified errors and potential improvements in the Flask application's `/synthesize_speech` endpoint to correctly utilize native voices for Spanish and French instead of defaulting to English.
   - Discussed and proposed corrections to the voice selection logic to match the requested language with the appropriate Google Cloud Text-to-Speech voice names.

3. **Next Steps**:
   1. Resolve the issue where the application does not utilize the specified native voices (Spanish or French) for speech synthesis and instead uses an English voice.
   2. Fix the frontend JavaScript code to ensure the "‚è≥ Processing..." icon is displayed correctly while the application is processing the audio for transcription and translation.

### app.py
from flask import Flask, request, jsonify, Response
import requests
from google.cloud import speech, texttospeech
from pydub import AudioSegment
import io
import logging
from datetime import datetime  # Add this import at the beginning of your file
from google.cloud import texttospeech

logging.basicConfig(level=logging.DEBUG)

processed_sessions = set()
app = Flask(__name__)

# Initialize Google Cloud Speech and Text-to-Speech clients
speech_client = speech.SpeechClient()
tts_client = texttospeech.TextToSpeechClient()

@app.route('/upload_audio', methods=['POST'])
def upload_audio(audio_file=None):
    if not audio_file:
        if 'audio' not in request.files:
            return jsonify({"message": "No audio file in request"}), 400
        audio_file = request.files['audio']

    audio_data = audio_file.read()
    audio_segment = AudioSegment.from_file(io.BytesIO(audio_data)).set_channels(1)

    if audio_segment.sample_width != 2:
        audio_segment = audio_segment.set_sample_width(2)
    if audio_segment.frame_rate != 16000:
        audio_segment = audio_segment.set_frame_rate(16000)

    audio_bytes = audio_segment.raw_data

    audio = speech.RecognitionAudio(content=audio_bytes)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        audio_channel_count=1,
        enable_automatic_punctuation=True
    )

    try:
        response = speech_client.recognize(config=config, audio=audio)
        transcript = response.results[0].alternatives[0].transcript
        return jsonify({"transcript": transcript})
    except Exception as e:
        return jsonify({"message": "Error processing audio", "error": str(e)}), 500

language_code_map = {
    'Spanish': 'es',  # Spanish language code
    'French': 'fr',   # Corrected French language code
    # Add other languages and their codes as needed
}


@app.route('/translate_text', methods=['POST'])
def translate_text():
    data = request.get_json(force=True)

    # Logging the received data for debugging
    logging.debug(f"Received data for translation: {data}")

    text_to_translate = data.get('text')
    target_language = data.get('language').capitalize()

    # Check if the target language is supported
    if target_language not in language_code_map:
        return jsonify({"message": f"Unsupported target language: {target_language}"}), 400

    target_language_code = language_code_map[target_language]

    # Logging the target language code
    logging.debug(f"Target language code: {target_language_code}")

    deepL_api_key = ''  
    url = "https://api-free.deepl.com/v2/translate"

    params = {
        'auth_key': deepL_api_key,
        'text': text_to_translate,
        'target_lang': target_language_code
    }

    response = requests.post(url, data=params)

    # Logging the response from the DeepL API
    logging.debug(f"DeepL API Response: {response.status_code}, {response.text}")

    if response.status_code == 200:
        translated_text = response.json()['translations'][0]['text']
        return jsonify({"translated_text": translated_text})
    else:
        error_msg = f"DeepL API error: {response.status_code} - {response.text}"
        logging.error(error_msg)
        return jsonify({"message": "Translation failed", "error": error_msg}), response.status_code


@app.route('/synthesize_speech', methods=['POST'])
def synthesize_speech():
    data = request.get_json()
    text = data.get('text')
    # Retrieve the high-level language selection (e.g., 'es' for Spanish, 'fr' for French)
    selected_language = data.get('language', 'en-US')

    # Define the mapping with corrected language codes
    tts_language_voice_map = {
        'es': ('es-ES', 'es-ES-Neural2-F'),  # Spanish voice
        'fr': ('fr-FR', 'fr-FR-Neural2-B'),  # French voice
    }

    # Get the detailed language code and voice name, default to US English if not found
    language_code, voice_name = tts_language_voice_map.get(selected_language, ('en-US', 'en-US-Standard-B'))

    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(
        language_code=language_code,
        name=voice_name,
        ssml_gender=texttospeech.SsmlVoiceGender.MALE
    )
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

    try:
        response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        if len(response.audio_content) == 0:
            logging.error("No audio content generated by the Text-to-Speech API.")
            return jsonify({"message": "Error synthesizing speech: No audio content"}), 500

        return Response(response.audio_content, mimetype='audio/mp3')
    except Exception as e:
        logging.error(f"Error in text-to-speech synthesis: {e}")
        return jsonify({"message": "Error in text-to-speech synthesis", "error": str(e)}), 500



@app.route('/process_audio', methods=['POST'])
def process_audio():
    session_id = request.form.get('recordingSessionId')
    if session_id in processed_sessions:
        return jsonify({"message": "This audio has already been processed."}), 400
    
    processed_sessions.add(session_id)
    logging.debug("process_audio endpoint called") 
    if 'audio' not in request.files or 'language' not in request.form:
        logging.error("Audio file or language not provided in the request.")
        return jsonify({"message": "Audio file or language not provided"}), 400

    audio_file = request.files['audio']
    selected_language = request.form['language']

    # Debug log for received audio file and selected language
    logging.debug(f"Received audio file: {audio_file.filename}")
    logging.debug(f"Selected language: {selected_language}")
    

    try:
        # Save the received audio file for debugging
        # temp_audio_filename = f"temp_audio_{datetime.now().strftime('%Y%m%d%H%M%S')}.wav"
        # logging.debug(f"Attempting to save audio file: {temp_audio_filename}")
        # try:
        #     audio_file.save(temp_audio_filename)
        #     logging.debug(f"Audio file saved: {temp_audio_filename}")
        # except Exception as e:
        #     logging.error(f"Error saving audio file: {e}")
        #     return jsonify({"message": f"Error saving audio file: {str(e)}"}), 500

        audio_file.seek(0)  # Reset file read pointer
        audio_data = audio_file.read()
        logging.debug(f"Size of audio data: {len(audio_data)} bytes")

        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data)).set_channels(1)
        logging.debug(f"Audio Segment: Channels - {audio_segment.channels}, Frame Rate - {audio_segment.frame_rate}, Sample Width - {audio_segment.sample_width}")

        if audio_segment.sample_width != 2:
            audio_segment = audio_segment.set_sample_width(2)
        if audio_segment.frame_rate != 16000:
            audio_segment = audio_segment.set_frame_rate(16000)


        audio_bytes = audio_segment.raw_data
        audio = speech.RecognitionAudio(content=audio_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="en-US",
            audio_channel_count=1,
            enable_automatic_punctuation=True
        )

        response = speech_client.recognize(config=config, audio=audio)

        if not response.results:
            logging.error("No transcription results.")
            return jsonify({"message": "Error in transcription: No results"}), 500

        transcript = response.results[0].alternatives[0].transcript
        logging.debug(f"Transcript (English): {transcript}")
        
        transcript = response.results[0].alternatives[0].transcript

        logging.debug(f"Transcript (English): {transcript}")

        # Translate the transcript
        translation_response = requests.post('http://localhost:5000/translate_text', json={'text': transcript, 'language': selected_language})
        if translation_response.status_code != 200:
            return jsonify({"message": "Error translating text"}), translation_response.status_code
        translated_text = translation_response.json()['translated_text']

        logging.debug(f"Translated text (Target Language - {selected_language}): {translated_text}")

        # Synthesize speech from the translated text
        speech_response = requests.post('http://localhost:5000/synthesize_speech', json={'text': translated_text, 'language': selected_language})
        if speech_response.status_code != 200:
            return jsonify({"message": "Error synthesizing speech"}), speech_response.status_code
        audio_content = speech_response.content

        logging.debug(f"Type of audio content: {type(audio_content)}")
        logging.debug(f"Length of audio content: {len(audio_content)}")
        logging.debug(f"Transcription and translation completed successfully.")

        return jsonify({"transcript": transcript, "translated_text": translated_text})

    except Exception as e:
        logging.error(f"Error in processing audio: {e}")
        return jsonify({"message": "Error in processing audio", "error": str(e)}), 500


@app.route('/')
def index():
    return app.send_static_file('index.html')


if __name__ == '__main__':
    app.run(debug=True)

### script.js 
document.addEventListener('DOMContentLoaded', function() {
    const startRecordBtn = document.getElementById('start-record-btn');
    const transcribedTextBox = document.getElementById('transcribed-text-box');
    const translatedTextBox = document.getElementById('translated-text-box');
    const processingIndicator = document.getElementById('processingIndicator');
    const playbackIndicator = document.getElementById('playbackIndicator');

    let isRecording = false;
    let mediaRecorder;
    let audioChunks = [];
    let currentAudio = null;
    let isSynthesizingSpeech = false; // Flag to prevent duplicate speech synthesis

    function generateUniqueId() {
        return Date.now().toString(36) + Math.random().toString(36).substring(2);
    }

    startRecordBtn.addEventListener('click', function() {
        if (!isRecording) {
            startRecording();
        } else {
            stopRecording();
        }
        isRecording = !isRecording;
    });

    function startRecording() {
        audioChunks = [];
        const recordingSessionId = generateUniqueId(); // Generate a unique ID for this session
        sessionStorage.setItem('recordingSessionId', recordingSessionId);

        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(stream => {
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.start();
                document.getElementById('recordingIndicator').style.display = 'block';

                mediaRecorder.ondataavailable = function(e) {
                    audioChunks.push(e.data);
                };
            });
        startRecordBtn.textContent = 'Stop Recording';
    }

    function stopRecording() {
        mediaRecorder.stop();
        startRecordBtn.textContent = 'Start Recording';
        document.getElementById('recordingIndicator').style.display = 'none';
        mediaRecorder.onstop = sendAudioToServer; // Call sendAudioToServer when the recording stops
        mediaRecorder.stream.getTracks().forEach(track => track.stop()); // Stop the media stream
    }

    function sendAudioToServer() {
        processingIndicator.style.display = 'block';

        const audioBlob = new Blob(audioChunks, {type: 'audio/wav'});
        const formData = new FormData();
        formData.append('audio', audioBlob);
        const recordingSessionId = sessionStorage.getItem('recordingSessionId');
        formData.append('recordingSessionId', recordingSessionId); // Append the session ID to the form data
        const selectedLanguage = document.getElementById('language-dropdown').value;
        formData.append('language', selectedLanguage);

        fetch('http://127.0.0.1:5000/process_audio', {
            method: 'POST',
            body: formData,
        }).then(response => response.json())
        .then(data => {
            processingIndicator.style.display = 'none';
            transcribedTextBox.textContent = data.transcript;
            translatedTextBox.textContent = data.translated_text;
            playSynthesizedSpeech(data.translated_text, selectedLanguage);
        }).catch(error => {
            console.error('Error:', error);
            processingIndicator.style.display = 'none';
        });
    }

    function playSynthesizedSpeech(text, language) {
        if (isSynthesizingSpeech) {
            console.log("A speech synthesis request is already in progress.");
            return;
        }
        isSynthesizingSpeech = true;

        if (currentAudio && !currentAudio.paused) {
            currentAudio.pause();
            currentAudio.currentTime = 0;
        }

        fetch('http://127.0.0.1:5000/synthesize_speech', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({text: text, language: language}),
        }).then(response => response.blob())
        .then(blob => {
            const audioUrl = URL.createObjectURL(blob);
            currentAudio = new Audio(audioUrl);
            playbackIndicator.style.display = 'block';
            currentAudio.play();
            currentAudio.onended = () => {
                playbackIndicator.style.display = 'none';
                isSynthesizingSpeech = false; // Reset the flag when playback ends
            };
        }).catch(error => {
            console.error('Error:', error);
            playbackIndicator.style.display = 'none';
            isSynthesizingSpeech = false; // Reset the flag in case of error
        });
    }
});


### index.html  
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multilingual Speech App</title>
    <link rel="stylesheet" href="/static/style.css">
    <script src="/static/script.js"></script>

</head>
<body>

    <button id="start-record-btn">Start Recording</button>
    <div id="recordingIndicator" style="display:none;">üî¥ Recording...</div>
    <div id="processingIndicator" style="display:none;">‚è≥ Processing...</div>
    <div id="playbackIndicator" style="display:none;">üîä Playing...</div>

    <div>
        <h2>Transcribed Text:</h2>
        <div id="transcribed-text-box"></div>
    </div>

    <div>
        <h2>Translated Text:</h2>
        <select id="language-dropdown" onchange="handleLanguageChange()">
            <option value="Spanish">Spanish</option>
            <option value="French">French</option>
        </select>
        <div id="translated-text-box"></div>
    </div>

    <script src="/static/script.js"></script>
    
</body>
</html>


### stle.css
body {
  background-color: #121212; /* Very dark gray, almost black */
  color: #E0E0E0; /* Light grey for text */
  font-family: 'Roboto', sans-serif;
  line-height: 1.6;
}

button, select {
  background-color: #0D47A1; /* Vibrant Blue for buttons and dropdowns */
  color: #fff;
  border: none;
  padding: 10px 15px;
  margin: 5px 0;
  border-radius: 4px;
  cursor: pointer;
  font-family: 'Roboto', sans-serif;
  transition: background-color 0.3s;
}

button:hover, select:hover {
  background-color: #0D47A1; /* Slightly darker blue on hover */
}

#transcribed-text-box, #translated-text-box {
  background-color: #1E1E1E; /* Slightly lighter than body for contrast */
  border: 1px solid #333;
  color: #E0E0E0; /* Light grey text for readability */
  padding: 15px;
  min-height: 100px;
  overflow-y: auto;
  border-radius: 4px;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  font-family: 'Courier New', monospace;
}

/* Styling for dropdown options */
option {
  background-color: #121212; /* Same as body for a cohesive look */
  color: #E0E0E0; /* Light grey for readability */
}


