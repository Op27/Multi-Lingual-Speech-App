/start
- We are deveploping an application. First, we want to change the speech to native Spanish or French speakers as currently it seems English speaker making speeches in those two languages. Second, we have to solve the error messages displayed in the inspector tool and command prompt. See the details below about error messages. 

Errors in inspector tool:
script.js:64 
        
        
       POST http://127.0.0.1:5000/process_audio 400 (BAD REQUEST)
sendAudioToServer @ script.js:64
script.js:91 
        
        
       POST http://127.0.0.1:5000/synthesize_speech 500 (INTERNAL SERVER ERROR)
playSynthesizedSpeech @ script.js:91
(anonymous) @ script.js:72
Promise.then (async)
sendAudioToServer @ script.js:68
127.0.0.1/:1 Uncaught (in promise) DOMException: Failed to load because no supported source was found.
content.js:47 Download the Apollo DevTools for a better development experience: https://chrome.google.com/webstore/detail/apollo-client-developer-t/jdkknkkbebbapilgoeccciglkfbmbnfm
favicon.ico:1 
        
        
       GET http://127.0.0.1:5000/favicon.ico 404 (NOT FOUND)

Logs in command prompt:
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:20] "GET / HTTP/1.1" 304 -
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:21] "GET /static/style.css HTTP/1.1" 304 -
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:21] "GET /static/script.js HTTP/1.1" 304 -
DEBUG:root:process_audio endpoint called
DEBUG:root:Received audio file: blob
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:27] "POST /process_audio HTTP/1.1" 400 -
DEBUG:root:Selected language: Spanish
DEBUG:root:Size of audio data: 85883 bytes
DEBUG:pydub.converter:subprocess.call(['ffmpeg', '-y', '-read_ahead_limit', '-1', '-i', 'cache:pipe:0', '-acodec', 'pcm_s32le', '-vn', '-f', 'wav', '-'])
DEBUG:root:Audio Segment: Channels - 1, Frame Rate - 48000, Sample Width - 4
ERROR:root:Error in text-to-speech synthesis: 400 Input type has to be text or SSML. Maybe the input is empty?
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:28] "POST /synthesize_speech HTTP/1.1" 500 -
DEBUG:root:Transcript (English): This is an error message. This is a pen. This is an apple.
DEBUG:root:Transcript (English): This is an error message. This is a pen. This is an apple.
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000
DEBUG:root:Received data for translation: {'text': 'This is an error message. This is a pen. This is an apple.', 'language': 'Spanish'}
DEBUG:root:Target language code: es
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api-free.deepl.com:443
DEBUG:urllib3.connectionpool:https://api-free.deepl.com:443 "POST /v2/translate HTTP/1.1" 200 None
DEBUG:root:DeepL API Response: 200, {"translations":[{"detected_source_language":"EN","text":"Esto es un mensaje de error. Esto es un bol√≠grafo. Esto es una manzana."}]}
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:33] "POST /translate_text HTTP/1.1" 200 -
DEBUG:urllib3.connectionpool:http://localhost:5000 "POST /translate_text HTTP/1.1" 200 99
DEBUG:root:Translated text (Target Language - Spanish): Esto es un mensaje de error. Esto es un bol√≠grafo. Esto es una manzana.
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:35] "POST /synthesize_speech HTTP/1.1" 200 -
DEBUG:urllib3.connectionpool:http://localhost:5000 "POST /synthesize_speech HTTP/1.1" 200 27840
DEBUG:root:Type of audio content: <class 'bytes'>
DEBUG:root:Length of audio content: 27840
DEBUG:root:Transcription and translation completed successfully.
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:35] "POST /process_audio HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:35] "POST /synthesize_speech HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [06/Feb/2024 15:45:35] "GET /favicon.ico HTTP/1.1" 404 -


Conditions:
- See below details about the purpuse of the application, current status, and key issues to address. 
- In your response, please use "we" as this is a collaborative work. 
- In your every response, please start by the professor synapse icon "üßô‚Äç‚ôÇÔ∏è". 
- To avoid mistakes, please suggest one changes/addition at each time so that I can test the changes. 
- we have four separate scripts for this application, app.py, script.js, index.html and style.css. See the latest codes below for your reference.  
- When we might going to change the code significantly, give me a heads-up when suggesting changes so that I will save the previous send_static_file. 
 
1. Purpose of the Application:
   - Developing a multilingual speech application with a Flask backend.
   - Functionality includes recording audio in English, transcribing it, translating the transcription into selected languages (currently Spanish, French, and planning to add Ukrainian), and synthesizing speech from the translated text.

2. Key issues to address
    1. Non-native voices are used when translated texts are pronounced. We want to change the speech to native Spanish or French speakers as currently it seems English speaker making speeches in those two languages. 
    2. There are errors in inspectors tools. We have to solve the error messages displayed in the inspector tool and command prompt. 


###app.py
from flask import Flask, request, jsonify, Response
import requests
from google.cloud import speech, texttospeech
from pydub import AudioSegment
import io
import logging
from datetime import datetime  # Add this import at the beginning of your file
from google.cloud import texttospeech

logging.basicConfig(level=logging.DEBUG)

processed_sessions = set()
app = Flask(__name__)

# Initialize Google Cloud Speech and Text-to-Speech clients
speech_client = speech.SpeechClient()
tts_client = texttospeech.TextToSpeechClient()

@app.route('/upload_audio', methods=['POST'])
def upload_audio(audio_file=None):
    if not audio_file:
        if 'audio' not in request.files:
            return jsonify({"message": "No audio file in request"}), 400
        audio_file = request.files['audio']

    audio_data = audio_file.read()
    audio_segment = AudioSegment.from_file(io.BytesIO(audio_data)).set_channels(1)

    if audio_segment.sample_width != 2:
        audio_segment = audio_segment.set_sample_width(2)
    if audio_segment.frame_rate != 16000:
        audio_segment = audio_segment.set_frame_rate(16000)

    audio_bytes = audio_segment.raw_data

    audio = speech.RecognitionAudio(content=audio_bytes)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        audio_channel_count=1,
        enable_automatic_punctuation=True
    )

    try:
        response = speech_client.recognize(config=config, audio=audio)
        transcript = response.results[0].alternatives[0].transcript
        return jsonify({"transcript": transcript})
    except Exception as e:
        return jsonify({"message": "Error processing audio", "error": str(e)}), 500

language_code_map = {
    'Spanish': 'es',  # Spanish language code
    'French': 'fr',   # Corrected French language code
    # Add other languages and their codes as needed
}


@app.route('/translate_text', methods=['POST'])
def translate_text():
    data = request.get_json(force=True)

    # Logging the received data for debugging
    logging.debug(f"Received data for translation: {data}")

    text_to_translate = data.get('text')
    target_language = data.get('language').capitalize()

    # Check if the target language is supported
    if target_language not in language_code_map:
        return jsonify({"message": f"Unsupported target language: {target_language}"}), 400

    target_language_code = language_code_map[target_language]

    # Logging the target language code
    logging.debug(f"Target language code: {target_language_code}")

    deepL_api_key = '3bf6993a-c2bf-f760-715e-dd6f4024b659:fx'  # Your DeepL API key
    url = "https://api-free.deepl.com/v2/translate"

    params = {
        'auth_key': deepL_api_key,
        'text': text_to_translate,
        'target_lang': target_language_code
    }

    response = requests.post(url, data=params)

    # Logging the response from the DeepL API
    logging.debug(f"DeepL API Response: {response.status_code}, {response.text}")

    if response.status_code == 200:
        translated_text = response.json()['translations'][0]['text']
        return jsonify({"translated_text": translated_text})
    else:
        error_msg = f"DeepL API error: {response.status_code} - {response.text}"
        logging.error(error_msg)
        return jsonify({"message": "Translation failed", "error": error_msg}), response.status_code

tts_language_voice_map = {
        'es': 'es-ES-Standard-B',  # Spanish male voice
        'fr': 'fr-FR-Standard-B',  # French male voice
        # Add other mappings as needed
    }

@app.route('/synthesize_speech', methods=['POST'])
def synthesize_speech():
    data = request.get_json()
    text = data.get('text')
    language_code = data.get('language', 'en-US')  # Default to US English if not provided

    # Correctly select the voice based on the language code
    if language_code == 'es':
        voice_name = 'es-ES-Standard-A'  # For Spanish, select a native speaker voice
    elif language_code == 'fr':
        voice_name = 'fr-FR-Standard-A'  # For French, select a native speaker voice
    else:
        voice_name = 'en-US-Standard-B'  # Default to a US English male voice

    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(
        language_code=language_code,
        name=voice_name,
        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
    )
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

    try:
        response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        if len(response.audio_content) == 0:
            logging.error("No audio content generated by the Text-to-Speech API.")
            return jsonify({"message": "Error synthesizing speech: No audio content"}), 500
        
        return Response(response.audio_content, mimetype='audio/mp3')
    except Exception as e:
        logging.error(f"Error in text-to-speech synthesis: {e}")
        return jsonify({"message": "Error in text-to-speech synthesis", "error": str(e)}), 500



@app.route('/process_audio', methods=['POST'])
def process_audio():
    session_id = request.form.get('recordingSessionId')
    if session_id in processed_sessions:
        return jsonify({"message": "This audio has already been processed."}), 400
    
    processed_sessions.add(session_id)
    logging.debug("process_audio endpoint called") 
    if 'audio' not in request.files or 'language' not in request.form:
        logging.error("Audio file or language not provided in the request.")
        return jsonify({"message": "Audio file or language not provided"}), 400

    audio_file = request.files['audio']
    selected_language = request.form['language']

    # Debug log for received audio file and selected language
    logging.debug(f"Received audio file: {audio_file.filename}")
    logging.debug(f"Selected language: {selected_language}")
    

    try:
        # Save the received audio file for debugging
        # temp_audio_filename = f"temp_audio_{datetime.now().strftime('%Y%m%d%H%M%S')}.wav"
        # logging.debug(f"Attempting to save audio file: {temp_audio_filename}")
        # try:
        #     audio_file.save(temp_audio_filename)
        #     logging.debug(f"Audio file saved: {temp_audio_filename}")
        # except Exception as e:
        #     logging.error(f"Error saving audio file: {e}")
        #     return jsonify({"message": f"Error saving audio file: {str(e)}"}), 500

        audio_file.seek(0)  # Reset file read pointer
        audio_data = audio_file.read()
        logging.debug(f"Size of audio data: {len(audio_data)} bytes")

        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data)).set_channels(1)
        logging.debug(f"Audio Segment: Channels - {audio_segment.channels}, Frame Rate - {audio_segment.frame_rate}, Sample Width - {audio_segment.sample_width}")

        if audio_segment.sample_width != 2:
            audio_segment = audio_segment.set_sample_width(2)
        if audio_segment.frame_rate != 16000:
            audio_segment = audio_segment.set_frame_rate(16000)


        audio_bytes = audio_segment.raw_data
        audio = speech.RecognitionAudio(content=audio_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="en-US",
            audio_channel_count=1,
            enable_automatic_punctuation=True
        )

        response = speech_client.recognize(config=config, audio=audio)

        if not response.results:
            logging.error("No transcription results.")
            return jsonify({"message": "Error in transcription: No results"}), 500

        transcript = response.results[0].alternatives[0].transcript
        logging.debug(f"Transcript (English): {transcript}")
        
        transcript = response.results[0].alternatives[0].transcript

        logging.debug(f"Transcript (English): {transcript}")

        # Translate the transcript
        translation_response = requests.post('http://localhost:5000/translate_text', json={'text': transcript, 'language': selected_language})
        if translation_response.status_code != 200:
            return jsonify({"message": "Error translating text"}), translation_response.status_code
        translated_text = translation_response.json()['translated_text']

        logging.debug(f"Translated text (Target Language - {selected_language}): {translated_text}")

        # Synthesize speech from the translated text
        speech_response = requests.post('http://localhost:5000/synthesize_speech', json={'text': translated_text, 'language': selected_language})
        if speech_response.status_code != 200:
            return jsonify({"message": "Error synthesizing speech"}), speech_response.status_code
        audio_content = speech_response.content

        logging.debug(f"Type of audio content: {type(audio_content)}")
        logging.debug(f"Length of audio content: {len(audio_content)}")
        logging.debug(f"Transcription and translation completed successfully.")

        return jsonify({"transcript": transcript, "translated_text": translated_text})

    except Exception as e:
        logging.error(f"Error in processing audio: {e}")
        return jsonify({"message": "Error in processing audio", "error": str(e)}), 500


@app.route('/')
def index():
    return app.send_static_file('index.html')


if __name__ == '__main__':
    app.run(debug=True)

###script.js
document.addEventListener('DOMContentLoaded', function() {
    const startRecordBtn = document.getElementById('start-record-btn');
    const transcribedTextBox = document.getElementById('transcribed-text-box');
    const translatedTextBox = document.getElementById('translated-text-box');
    const processingIndicator = document.getElementById('processingIndicator');
    const playbackIndicator = document.getElementById('playbackIndicator');

    let isRecording = false;
    let mediaRecorder;
    let audioChunks = [];
    let currentAudio = null;
    let isSynthesizingSpeech = false; // Flag to prevent duplicate speech synthesis

    function generateUniqueId() {
        return Date.now().toString(36) + Math.random().toString(36).substring(2);
    }

    startRecordBtn.addEventListener('click', function() {
        if (!isRecording) {
            startRecording();
        } else {
            stopRecording();
        }
        isRecording = !isRecording;
    });

    function startRecording() {
        audioChunks = [];
        const recordingSessionId = generateUniqueId(); // Generate a unique ID for this session
        sessionStorage.setItem('recordingSessionId', recordingSessionId);

        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(stream => {
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.start();
                document.getElementById('recordingIndicator').style.display = 'block';

                mediaRecorder.ondataavailable = function(e) {
                    audioChunks.push(e.data);
                };
            });
        startRecordBtn.textContent = 'Stop Recording';
    }

    function stopRecording() {
        mediaRecorder.stop();
        startRecordBtn.textContent = 'Start Recording';
        document.getElementById('recordingIndicator').style.display = 'none';
        mediaRecorder.onstop = sendAudioToServer; // Call sendAudioToServer when the recording stops
        mediaRecorder.stream.getTracks().forEach(track => track.stop()); // Stop the media stream
    }

    function sendAudioToServer() {
        processingIndicator.style.display = 'block';

        const audioBlob = new Blob(audioChunks, {type: 'audio/wav'});
        const formData = new FormData();
        formData.append('audio', audioBlob);
        const recordingSessionId = sessionStorage.getItem('recordingSessionId');
        formData.append('recordingSessionId', recordingSessionId); // Append the session ID to the form data
        const selectedLanguage = document.getElementById('language-dropdown').value;
        formData.append('language', selectedLanguage);

        fetch('http://127.0.0.1:5000/process_audio', {
            method: 'POST',
            body: formData,
        }).then(response => response.json())
        .then(data => {
            processingIndicator.style.display = 'none';
            transcribedTextBox.textContent = data.transcript;
            translatedTextBox.textContent = data.translated_text;
            playSynthesizedSpeech(data.translated_text, selectedLanguage);
        }).catch(error => {
            console.error('Error:', error);
            processingIndicator.style.display = 'none';
        });
    }

    function playSynthesizedSpeech(text, language) {
        if (isSynthesizingSpeech) {
            console.log("A speech synthesis request is already in progress.");
            return;
        }
        isSynthesizingSpeech = true;

        if (currentAudio && !currentAudio.paused) {
            currentAudio.pause();
            currentAudio.currentTime = 0;
        }

        fetch('http://127.0.0.1:5000/synthesize_speech', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({text: text, language: language}),
        }).then(response => response.blob())
        .then(blob => {
            const audioUrl = URL.createObjectURL(blob);
            currentAudio = new Audio(audioUrl);
            playbackIndicator.style.display = 'block';
            currentAudio.play();
            currentAudio.onended = () => {
                playbackIndicator.style.display = 'none';
                isSynthesizingSpeech = false; // Reset the flag when playback ends
            };
        }).catch(error => {
            console.error('Error:', error);
            playbackIndicator.style.display = 'none';
            isSynthesizingSpeech = false; // Reset the flag in case of error
        });
    }
});


###index.html 
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multilingual Speech App</title>
    <link rel="stylesheet" href="/static/style.css">
    <script src="/static/script.js"></script>

</head>
<body>

    <button id="start-record-btn">Start Recording</button>
    <div id="recordingIndicator" style="display:none;">üî¥ Recording...</div>
    <div id="processingIndicator" style="display:none;">‚è≥ Processing...</div>
    <div id="playbackIndicator" style="display:none;">üîä Playing...</div>

    <div>
        <h2>Transcribed Text:</h2>
        <div id="transcribed-text-box"></div>
    </div>

    <div>
        <h2>Translated Text:</h2>
        <select id="language-dropdown" onchange="handleLanguageChange()">
            <option value="Spanish">Spanish</option>
            <option value="French">French</option>
        </select>
        <div id="translated-text-box"></div>
    </div>

    <script src="/static/script.js"></script>
    
</body>
</html>



###style.css
body {
  background-color: #121212; /* Very dark gray, almost black */
  color: #E0E0E0; /* Light grey for text */
  font-family: 'Roboto', sans-serif;
  line-height: 1.6;
}

button, select {
  background-color: #0D47A1; /* Vibrant Blue for buttons and dropdowns */
  color: #fff;
  border: none;
  padding: 10px 15px;
  margin: 5px 0;
  border-radius: 4px;
  cursor: pointer;
  font-family: 'Roboto', sans-serif;
  transition: background-color 0.3s;
}

button:hover, select:hover {
  background-color: #0D47A1; /* Slightly darker blue on hover */
}

#transcribed-text-box, #translated-text-box {
  background-color: #1E1E1E; /* Slightly lighter than body for contrast */
  border: 1px solid #333;
  color: #E0E0E0; /* Light grey text for readability */
  padding: 15px;
  min-height: 100px;
  overflow-y: auto;
  border-radius: 4px;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  font-family: 'Courier New', monospace;
}

/* Styling for dropdown options */
option {
  background-color: #121212; /* Same as body for a cohesive look */
  color: #E0E0E0; /* Light grey for readability */
}

