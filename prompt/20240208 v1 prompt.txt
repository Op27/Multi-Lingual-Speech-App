/start
- Good morning! Let's continue enhancing the text-to-speech functionality of our transcription and translation application by focusing on making the synthesized speech sound more like a native Spanish or French speaker. Additionally, we'll finalize the project and prepare it for publication on GitHub.
- See below for detals about Purpose of the Application, Achieved Tasks, and Next Steps, 
- In your response, please use "we" as this is a collaborative work. 
- In your every response, please start by the professor synapse icon "üßô‚Äç‚ôÇÔ∏è". 
- To avoid mistakes, please suggest one changes/addition at each time so that I can test the changes. 
- we have four separate scripts for this application, app.py, script.js, index.html and stle.css. See the latest codes to comprehend the structure.  
 
1. Purpose of the application:
- The application is designed to transcribe audio recordings, translate the transcriptions into a specified language (e.g., Spanish, French), and synthesize the translated text into speech.

2. Achieved tasks:
- Successfully implemented audio recording and processing to transcribe spoken words.
- Implemented translation of transcriptions into specified languages using an API.
- Integrated text-to-speech synthesis to vocalize translated texts.
- Resolved issues related to multiple voices being pronounced simultaneously and ensured only one voice is heard at a time.
- Addressed errors related to audio playback and queue management.

3. Next steps:
- Enhance the text-to-speech feature to make the audio speech sound like a native speaker of the target language (Spanish or French) rather than an English person pronouncing these languages. This involves possibly adjusting or selecting different voice models or settings in the Google Text-to-Speech API to more closely match native pronunciation.
- Finalize the application, addressing any remaining bugs or enhancements, and publish the project to GitHub for public access and contribution.

4. Lessons learned from today's coding:
- Importance of debugging and iterative testing to resolve unforeseen issues.
- Techniques for managing audio playback queues to ensure smooth user experience.
- The value of clear error handling and logging for troubleshooting and monitoring application behavior.
- Strategies for making synthesized speech more natural and closely aligned with native speaker pronunciation.

#app.py 
from flask import Flask, request, jsonify, Response
import requests
from google.cloud import speech, texttospeech
from pydub import AudioSegment
import io
import logging
from datetime import datetime  # Add this import at the beginning of your file
from google.cloud import texttospeech

logging.basicConfig(level=logging.DEBUG)

processed_sessions = set()
app = Flask(__name__)

# Initialize Google Cloud Speech and Text-to-Speech clients
speech_client = speech.SpeechClient()
tts_client = texttospeech.TextToSpeechClient()

@app.route('/upload_audio', methods=['POST'])
def upload_audio(audio_file=None):
    if not audio_file:
        if 'audio' not in request.files:
            return jsonify({"message": "No audio file in request"}), 400
        audio_file = request.files['audio']

    audio_data = audio_file.read()
    audio_segment = AudioSegment.from_file(io.BytesIO(audio_data)).set_channels(1)

    if audio_segment.sample_width != 2:
        audio_segment = audio_segment.set_sample_width(2)
    if audio_segment.frame_rate != 16000:
        audio_segment = audio_segment.set_frame_rate(16000)

    audio_bytes = audio_segment.raw_data

    audio = speech.RecognitionAudio(content=audio_bytes)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="en-US",
        audio_channel_count=1,
        enable_automatic_punctuation=True
    )

    try:
        response = speech_client.recognize(config=config, audio=audio)
        transcript = response.results[0].alternatives[0].transcript
        return jsonify({"transcript": transcript})
    except Exception as e:
        return jsonify({"message": "Error processing audio", "error": str(e)}), 500

language_code_map = {
    'Spanish': 'es',  # Spanish language code
    'French': 'fr',   # Corrected French language code
    # Add other languages and their codes as needed
}


@app.route('/translate_text', methods=['POST'])
def translate_text():
    data = request.get_json(force=True)

    # Logging the received data for debugging
    logging.debug(f"Received data for translation: {data}")

    text_to_translate = data.get('text')
    target_language = data.get('language').capitalize()

    # Check if the target language is supported
    if target_language not in language_code_map:
        return jsonify({"message": f"Unsupported target language: {target_language}"}), 400

    target_language_code = language_code_map[target_language]

    # Logging the target language code
    logging.debug(f"Target language code: {target_language_code}")

    deepL_api_key = ''  # Your DeepL API key
    url = "https://api-free.deepl.com/v2/translate"

    params = {
        'auth_key': deepL_api_key,
        'text': text_to_translate,
        'target_lang': target_language_code
    }

    response = requests.post(url, data=params)

    # Logging the response from the DeepL API
    logging.debug(f"DeepL API Response: {response.status_code}, {response.text}")

    if response.status_code == 200:
        translated_text = response.json()['translations'][0]['text']
        return jsonify({"translated_text": translated_text})
    else:
        error_msg = f"DeepL API error: {response.status_code} - {response.text}"
        logging.error(error_msg)
        return jsonify({"message": "Translation failed", "error": error_msg}), response.status_code


@app.route('/synthesize_speech', methods=['POST'])
def synthesize_speech():
    data = request.get_json()
    text = data.get('text')

    selected_language = data.get('language', 'en-US')

    tts_language_voice_map = {
        'es': ('es-ES', 'es-ES-Neural2-F'),  # Spanish voice
        'fr': ('fr-FR', 'fr-FR-Neural2-B'),  # French voice
    }

    if not text:
        logging.error(f"Received empty 'text' for synthesis, data received: {data}")
        return jsonify({"message": "Error synthesizing speech: 'text' parameter is empty"}), 400
    else:
            logging.debug(f"Synthesizing speech for text: {text}")

    # Get the detailed language code and voice name, default to US English if not found
    language_code, voice_name = tts_language_voice_map.get(selected_language, ('en-US', 'en-US-Standard-B'))

    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(
        language_code=language_code,
        name=voice_name,
        ssml_gender=texttospeech.SsmlVoiceGender.MALE
    )
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

    try:
        response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        if len(response.audio_content) == 0:
            logging.error("No audio content generated by the Text-to-Speech API.")
            return jsonify({"message": "Error synthesizing speech: No audio content"}), 500

        return Response(response.audio_content, mimetype='audio/mp3')
    except Exception as e:
        logging.error(f"Error in text-to-speech synthesis: {e}")
        return jsonify({"message": "Error in text-to-speech synthesis", "error": str(e)}), 500



@app.route('/process_audio', methods=['POST'])
def process_audio():
    
    logging.debug(f"Audio in request: {'audio' in request.files}")
    logging.debug(f"Language in request: {'language' in request.form}")

    if 'audio' not in request.files:
        logging.error("No audio file part in the request")
        return jsonify({"message": "Audio file or language not provided"}), 400
    audio_file = request.files['audio']

    selected_language = request.form['language']

    if 'language' not in request.form:
        logging.error("No language part in the request")
        return jsonify({"message": "Audio file or language not provided"}), 400
    selected_language = request.form['language']
    
    logging.debug(f"Processing audio for language: {selected_language}")


    # Debug log for received audio file and selected language
    logging.debug(f"Received audio file: {audio_file.filename}")
    logging.debug(f"Selected language: {selected_language}")
    

    try:
        # Save the received audio file for debugging
        # temp_audio_filename = f"temp_audio_{datetime.now().strftime('%Y%m%d%H%M%S')}.wav"
        # logging.debug(f"Attempting to save audio file: {temp_audio_filename}")
        # try:
        #     audio_file.save(temp_audio_filename)
        #     logging.debug(f"Audio file saved: {temp_audio_filename}")
        # except Exception as e:
        #     logging.error(f"Error saving audio file: {e}")
        #     return jsonify({"message": f"Error saving audio file: {str(e)}"}), 500

        audio_file.seek(0)  # Reset file read pointer
        audio_data = audio_file.read()
        logging.debug(f"Size of audio data: {len(audio_data)} bytes")

        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data)).set_channels(1)
        logging.debug(f"Audio Segment: Channels - {audio_segment.channels}, Frame Rate - {audio_segment.frame_rate}, Sample Width - {audio_segment.sample_width}")

        if audio_segment.sample_width != 2:
            audio_segment = audio_segment.set_sample_width(2)
        if audio_segment.frame_rate != 16000:
            audio_segment = audio_segment.set_frame_rate(16000)

        logging.debug(f"Audio Segment: Channels - {audio_segment.channels}, Frame Rate - {audio_segment.frame_rate}, Sample Width - {audio_segment.sample_width}")

        audio_bytes = audio_segment.raw_data
        audio = speech.RecognitionAudio(content=audio_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="en-US",
            audio_channel_count=1,
            enable_automatic_punctuation=True
        )

        response = speech_client.recognize(config=config, audio=audio)

        if not response.results:
            logging.error("No transcription results.")
            return jsonify({"message": "Error in transcription: No results"}), 500

        transcript = response.results[0].alternatives[0].transcript
        logging.debug(f"Transcript (English): {transcript}")
        
        transcript = response.results[0].alternatives[0].transcript

        logging.debug(f"Transcript (English): {transcript}")

        # Translate the transcript
        translation_response = requests.post('http://localhost:5000/translate_text', json={'text': transcript, 'language': selected_language})
        if translation_response.status_code != 200:
            return jsonify({"message": "Error translating text"}), translation_response.status_code
        translated_text = translation_response.json()['translated_text']

        logging.debug(f"Translated text (Target Language - {selected_language}): {translated_text}")

        # Synthesize speech from the translated text
        speech_response = requests.post('http://localhost:5000/synthesize_speech', json={'text': translated_text, 'language': selected_language})
        if speech_response.status_code != 200:
            return jsonify({"message": "Error synthesizing speech"}), speech_response.status_code
        audio_content = speech_response.content

        logging.debug(f"Type of audio content: {type(audio_content)}")
        logging.debug(f"Length of audio content: {len(audio_content)}")
        logging.debug(f"Transcription and translation completed successfully.")

        return jsonify({"transcript": transcript, "translated_text": translated_text})

    except Exception as e:
        logging.error(f"Error in processing audio: {e}")
        return jsonify({"message": "Error in processing audio", "error": str(e)}), 500


@app.route('/')
def index():
    return app.send_static_file('index.html')


if __name__ == '__main__':
    app.run(debug=True)

#script.js 
document.addEventListener('DOMContentLoaded', function() {
    const startRecordBtn = document.getElementById('start-record-btn');
    const transcribedTextBox = document.getElementById('transcribed-text-box');
    const translatedTextBox = document.getElementById('translated-text-box');
    const processingIndicator = document.getElementById('processingIndicator');
    const playbackIndicator = document.getElementById('playbackIndicator');

    let isRecording = false;
    let mediaRecorder;
    let audioChunks = [];
    let currentAudio = null;
    let isSynthesizingSpeech = false; // Flag to prevent duplicate speech synthesis
    let audioQueue = []; // Initialize an empty queue

    function generateUniqueId() {
        return Date.now().toString(36) + Math.random().toString(36).substring(2);
    }
    
    function enqueueAudio(blob) {
        const audioUrl = URL.createObjectURL(blob);
        audioQueue.push(audioUrl);
    
        if (!isSynthesizingSpeech || audioQueue.length === 1) {
            playNextInQueue();
        }
    }  

    function playNextInQueue() {
        if (audioQueue.length > 0) {
            const nextAudioUrl = audioQueue.shift();
            currentAudio = new Audio(nextAudioUrl);
            currentAudio.play().then(() => {
                console.log("Audio playback started.");
            }).catch(error => {
                console.error("Error playing audio:", error);
            });
            currentAudio.onended = playNextInQueue;
        } else {
            isSynthesizingSpeech = false;
            console.log("Audio queue is empty.");
        }
    }

    startRecordBtn.addEventListener('click', function() {
        if (!isRecording) {
            startRecording();
        } else {
            stopRecording();
        }
        isRecording = !isRecording;
    });

    function startRecording() {
        audioChunks = [];
        const recordingSessionId = generateUniqueId(); // Generate a unique ID for this session
        sessionStorage.setItem('recordingSessionId', recordingSessionId);

        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(stream => {
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.start();
                document.getElementById('recordingIndicator').style.display = 'block';

                mediaRecorder.ondataavailable = function(e) {
                    audioChunks.push(e.data);
                };
            });
        startRecordBtn.textContent = 'Stop Recording';
    }

    function stopRecording() {
        mediaRecorder.stop();
        startRecordBtn.textContent = 'Start Recording';
        document.getElementById('recordingIndicator').style.display = 'none';
        mediaRecorder.onstop = sendAudioToServer; // Call sendAudioToServer when the recording stops
        mediaRecorder.stream.getTracks().forEach(track => track.stop()); // Stop the media stream
    }

    function sendAudioToServer() {
        processingIndicator.style.display = 'block';
    
        const audioBlob = new Blob(audioChunks, {type: 'audio/wav'});
        const selectedLanguage = document.getElementById('language-dropdown').value; // Correctly initialize selectedLanguage
        const formData = new FormData();
        formData.append('audio', audioBlob);
        formData.append('language', selectedLanguage); // Use the initialized selectedLanguage
    
        fetch('http://127.0.0.1:5000/process_audio', {
            method: 'POST',
            body: formData,
        }).then(response => response.json())
        .then(data => {
            processingIndicator.style.display = 'none';
            transcribedTextBox.textContent = data.transcript;
            translatedTextBox.textContent = data.translated_text;
            playSynthesizedSpeech(data.translated_text, selectedLanguage);
        }).catch(error => {
            console.error('Error:', error);
            processingIndicator.style.display = 'none';
        });
    }
        

    function playSynthesizedSpeech(text, language) {
        if (isSynthesizingSpeech) {
            console.log("A speech synthesis request is already in progress.");
            return;
        }
        isSynthesizingSpeech = true;
    
        // Directly fetch and enqueue the audio without intermediate handling
        fetch('http://127.0.0.1:5000/synthesize_speech', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({text: text, language: language}),
        }).then(response => response.blob())
        .then(blob => {
            enqueueAudio(blob); 
        }).catch(error => {
            console.error('Error:', error);
            isSynthesizingSpeech = false; 
        });
    }
       
});


#index.html 
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multilingual Speech App</title>
    <link rel="stylesheet" href="/static/style.css">
    <script src="/static/script.js"></script>

</head>
<body>

    <button id="start-record-btn">Start Recording</button>
    <div id="recordingIndicator" style="display:none;">üî¥ Recording...</div>
    <div id="processingIndicator" style="display:none;">‚è≥ Processing...</div>
    <div id="playbackIndicator" style="display:none;">üîä Playing...</div>

    <div>
        <h2>Transcribed Text:</h2>
        <div id="transcribed-text-box"></div>
    </div>

    <div>
        <h2>Translated Text:</h2>
        <select id="language-dropdown" onchange="handleLanguageChange()">
            <option value="Spanish">Spanish</option>
            <option value="French">French</option>
        </select>
        <div id="translated-text-box"></div>
    </div>

    <script src="/static/script.js"></script>
    
</body>
</html>


#stle.css
body {
  background-color: #121212; /* Very dark gray, almost black */
  color: #E0E0E0; /* Light grey for text */
  font-family: 'Roboto', sans-serif;
  line-height: 1.6;
}

button, select {
  background-color: #0D47A1; /* Vibrant Blue for buttons and dropdowns */
  color: #fff;
  border: none;
  padding: 10px 15px;
  margin: 5px 0;
  border-radius: 4px;
  cursor: pointer;
  font-family: 'Roboto', sans-serif;
  transition: background-color 0.3s;
}

button:hover, select:hover {
  background-color: #0D47A1; /* Slightly darker blue on hover */
}

#transcribed-text-box, #translated-text-box {
  background-color: #1E1E1E; /* Slightly lighter than body for contrast */
  border: 1px solid #333;
  color: #E0E0E0; /* Light grey text for readability */
  padding: 15px;
  min-height: 100px;
  overflow-y: auto;
  border-radius: 4px;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  font-family: 'Courier New', monospace;
}

/* Styling for dropdown options */
option {
  background-color: #121212; /* Same as body for a cohesive look */
  color: #E0E0E0; /* Light grey for readability */
}

